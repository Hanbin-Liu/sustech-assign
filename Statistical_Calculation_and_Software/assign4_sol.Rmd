---
title: "Statistical Calculation and Software"
author: "Assignment 4"
date: "Hanbin Liu 11912410"
output:
  pdf_document:
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
setwd("C:/Users/HP/Desktop/R_project/R_assignments")
```

## 4.1
```{r}
library(HSAUR3)
data(planets)
```
### (a)
```{r}
library(lattice)
library(viridisLite)
planets_dist <- dist(planets)
levelplot(
  as.matrix(planets_dist),
  xlab = "planets",
  ylab = "planets",
  col.regions = viridis(100),
  scales = list(draw = FALSE)
)


planets_single <- hclust(planets_dist, method = "single")
planets_complete <- hclust(planets_dist, method = "complete")
planets_average <- hclust(planets_dist, method = "average")

plot(planets_single,
     main = "Single Linkage",
     sub = "",
     xlab = "",
)
plot(planets_complete,
     main = "Complete Linkage",
     sub = "",
     xlab = "")
plot(planets_average,
     main = "Average Linkage",
     sub = "",
     xlab = "")

planets_cluster_single <- cutree(planets_single, h = 1000)
planets_cluster_complete <- cutree(planets_complete, h = 1500)
planets_cluster_average <- cutree(planets_average, h = 900)
planets_cluster_single
planets_cluster_complete
planets_cluster_average
```

### (b)
```{r}
## min-max standardized
rge <- apply(planets, 2, max) - apply(planets, 2, min)
planet.dat <-
  sweep(planets, 2, rge, FUN = "/") ### function = divide

## K=3
planet_kmeans3 <- kmeans(planet.dat, centers = 3)
planet_kmeans3
table(planet_kmeans3$cluster)

ccent <- function(cl) {
  f <- function(i)
    colMeans(planets[cl == i, ])
  x <- sapply(sort(unique(cl)), f)
  colnames(x) <- sort(unique(cl))
  return(x)
}

ccent(planet_kmeans3$cluster)

## K=5
planet_kmeans5 <- kmeans(planet.dat, centers = 5)
planet_kmeans5
table(planet_kmeans5$cluster)
ccent(planet_kmeans5$cluster)
```

- 3D scatterplot for K=3
```{r}
library("scatterplot3d")
layout(matrix(1))
scatterplot3d(
  log(planets$mass),
  log(planets$period),
  log(planets$eccen),
  type = "h",
  angle = 55,
  scale.y = 0.7,
  pch = planet_kmeans3$cluster,
  y.ticklabs = seq(0, 10, by = 2),
  y.margin.add = 0.1,
)
```

- 3D scatterplot for K=5
```{r}
library("scatterplot3d")
layout(matrix(1))
scatterplot3d(
  log(planets$mass),
  log(planets$period),
  log(planets$eccen),
  type = "h",
  angle = 55,
  scale.y = 0.7,
  pch = planet_kmeans5$cluster,
  y.ticklabs = seq(0, 10, by = 2),
  y.margin.add = 0.1,
)
```

### (c)
```{r}
logL <- function(param, x) {
  d1 <- dnorm(x, mean = param[2], sd = param[3])
  d2 <- dnorm(x, mean = param[4], sd = param[5])
  - sum(log(param[1] * d1 + (1 - param[1]) * d2))
}
x <- planets$eccen
startparam <-
  c(
    p = 0.5,
    mu1 = mean(x) / 2,
    sd1 = sd(x) / 2,
    mu2 = mean(x) * 2,
    sd2 = sd(x) * 2
  )

opp <-
  optim(
    startparam,
    logL,
    x = planets$eccen,
    method = "L-BFGS-B",
    lower = c(0.01, rep(0.01, 4)),
    upper = c(0.99, rep(1, 4))
  )

opp
```

### (d)
```{r}
library("mclust")
planet_mclust <- Mclust(planet.dat[3], G = 2)
print(planet_mclust)
table(planet_mclust$classification)

## sample statistics
ind1 <- planet_mclust$classification == 1
ind2 <- planet_mclust$classification == 2
data.frame(mclust.BIC = c(
  p = length(x[ind1]) / length(x),
  mu1 = mean(x[ind1]),
  sd1 = sd(x[ind1]),
  mu2 = mean(x[ind2]),
  sd2 = sd(x[ind2])
))
```
The results match the results in (c).
Compare:
```{r}
data.frame(loglikelihood = opp$par,
           mclust.BIC = c(21 / 101, mean(x[ind1]), sd(x[ind1]), mean(x[ind2]), sd(x[ind2])))
```

### (e)
```{r}
planets_pca <- prcomp(planets, scale = TRUE)
```
coefficients for the first two principal components:
```{r}
planets_pca$rotation[, 1]
planets_pca$rotation[, 2]
```
score:
```{r}
first.score <- predict(planets_pca)[, 1]
second.score <- predict(planets_pca)[, 2]
third.score <- predict(planets_pca)[, 3]

score <- data.frame(first = first.score,
                    second = second.score,
                    third = third.score)
score
```

### (f)
```{r}
## min-max standardized
rge <- apply(score, 2, max) - apply(score, 2, min)
score.dat <-
  sweep(score, 2, rge, FUN = "/") ### function = divide

## K=3
score_kmeans3 <- kmeans(score.dat[c(1, 2)], centers = 3)
```
compare:
```{r}
score_kmeans3
planet_kmeans3
```
```{r}
table(score_kmeans3$cluster)
table(planet_kmeans3$cluster)
```
The results are significantly different from the results in (b).

## 4.2
```{r}
library(ISLR)
data(Default)
```

### (a)
```{r}
## split the sample set
set.seed(1234)
train <- sample(nrow(Default), 0.7 * nrow(Default))
Default.train <- Default[train, ]
Default.validate <- Default[-train, ]

table(Default.train$default)
table(Default.validate$default)
```
```{r}
## logistic regression
fit.logit <-
  glm(default ~ student + balance + income,
      data = Default.train,
      family = binomial())
summary(fit.logit)
```
```{r}
prob <- predict(fit.logit, Default.validate, type = "response")

logit.pred <- factor(prob > .5,
                     levels = c(FALSE, TRUE),
                     labels = c("No", "Yes"))
logit.perf <- table(Default.validate$default,
                    logit.pred,
                    dnn = c("Actual", "Predicted"))

## confusion matrix
logit.perf

## validation set error
error <-
  (logit.perf[1, 2] + logit.perf[2, 1]) / (nrow(Default.validate))
error
```

### (b)
```{r}
library(rpart)
set.seed(1234)

dtree <- rpart(
  default ~ student + balance + income,
  data = Default.train,
  method = "class",
  parms = list(split = "information")
)

dtree$cptable
plotcp(dtree)
```
```{r}
dtree.pruned <- prune(dtree, cp = 0.039)
library(rpart.plot)
prp(
  dtree.pruned,
  type = 2,
  extra = 104,
  fallen.leaves = TRUE,
  main = "Decision Tree"
)
```

### (C)
```{r}
library(party)
fit.ctree <- ctree(default ~ student + balance + income,
                   data = Default.train)
plot(fit.ctree, main = "Conditional Inference Tree")

## prediction in the validation set
ctree.pred <-
  predict(fit.ctree, Default.validate, type = "response")

ctree.perf <- table(Default.validate$default,
                    ctree.pred,
                    dnn = c("Actual", "Predicted"))
## confusion matrix
ctree.perf
```

### (d)
- traditional decision trees:
```{r}
library(randomForest)
set.seed(1234)

## grow the forest
fit.forest1 <-
  randomForest(
    default ~ student + balance + income,
    data = Default.train,
    na.action = na.roughfix,
    importance = TRUE
  )
fit.forest1

forest1.pred <- predict(fit.forest1, Default.validate)
forest1.perf <- table(Default.validate$default,
                     forest1.pred,
                     dnn = c("Actual", "Predicted"))
## confusion matrix
forest1.perf
```

- conditional inference trees:
```{r}
## grow the forest
fit.forest2 <-
  cforest(default ~ student + balance + income,
          data = Default.train,
          controls = cforest_classical(mtry = 2))
fit.forest2

forest2.pred <- predict(fit.forest2, newdata = Default.validate)
forest2.perf <- table(Default.validate$default,
                      forest2.pred,
                      dnn = c("Actual", "Predicted"))
## confusion matrix
forest2.perf
```
Compare the predictive accuracy
```{r}
forest.traditional.accuracy  <-
  (forest1.perf[1, 1] + forest1.perf[2, 2]) / nrow(Default.validate)
forest.conditional.accuracy <-
  (forest2.perf[1, 1] + forest2.perf[2, 2]) / nrow(Default.validate)

data.frame(forest.traditional.accuracy, forest.conditional.accuracy)
```
The accuracy of random forest based on the conditional inference trees is slightly larger than the accuracy of random forest based on the traditional decision trees.


### (e)
```{r}
library(e1071)
set.seed(1234)
fit.svm <-
  svm(
    default ~ student + balance + income,
    data = Default.train,
    gamma = 1,
    cost = 1
  )
fit.svm

svm.pred <- predict(fit.svm, na.omit(Default.validate))
svm.perf <- table(na.omit(Default.validate)$default,
                  svm.pred,
                  dnn = c("Actual", "Predicted"))
svm.perf
```
- Compare:
```{r}
performance <- function(table, n = 2) {
  if (!all(dim(table) == c(2, 2)))
    stop("Must be a 2 x 2 table")
  tn <- table[1, 1]
  fp <- table[1, 2]
  fn <- table[2, 1]
  tp <- table[2, 2]
  sensitivity <- tp / (tp + fn)
  specificity <- tn / (tn + fp)
  ppp <- tp / (tp + fp)
  npp <- tn / (tn + fn)
  hitrate <- (tp + tn) / (tp + tn + fp + fn)
  result <-
    data.frame(c(sensitivity,
                 specificity,
                 ppp,
                 npp,
                 hitrate))
  return(result)
}


result <- data.frame(
  svm = performance(svm.perf),
  conditionaltree = performance(ctree.perf),
  forest1 = performance(forest1.perf),
  forest2 = performance(forest2.perf),
  logit = performance(logit.perf),
  row.names = c(
    "sensitivity",
    "specificity",
    "positie predictive power",
    "negative predicive power",
    "accuracy"
  )
)
colnames(result) <-
  c("svm", "cond.tree", "forest.trad", "forest.cond", "logit")
result
```
Conclusion: 

Random forest based on the conditional trees has the same performance as the logistic regression. They have the best sensitivity, negative predictive power, and accuracy.

While svm has the best specificity and positive predictive power.















