---
title: "Statistical Calculation and Software"
author: "Assignment 3"
date: "Hanbin Liu 11912410"
output:
  #html_document
  pdf_document:
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
setwd("C:/Users/HP/Desktop/R_project/R_assignments")
library(ggplot2)
```

## 3.1
```{r}
data(cars)
any(is.na(cars))
```
### (a)
```{r}
attach(cars)
```
```{r}
plot(speed, dist)
nw_box <- ksmooth(speed, dist, kernel = 'box', bandwidth = 1)
nw_box
nw_gaussian <-
  ksmooth(speed, dist, kernel = 'normal', bandwidth = 1)
nw_gaussian
```

### (b)
```{r}
tt1 <-
  loess(
    dist ~ speed,
    data = cars,
    span = 0.15,
    degree = 2,
    family = 'gaussian'
  )
tt2 <-
  loess(
    dist ~ speed,
    data = cars,
    span = 0.5,
    degree = 2,
    family = 'gaussian'
  )
```

### (c)
Nadaraya-Watson Kernel Regression model:
```{r}
##
nw_fitted_box <-
  ksmooth(
    speed,
    dist,
    kernel = 'box',
    bandwidth = 1,
    x.points = speed
  )$y
nw_fitted_box

##
nw_fitted_gaussian <-
  ksmooth(
    speed,
    dist,
    kernel = 'normal',
    bandwidth = 1,
    x.points = speed
  )$y
nw_fitted_gaussian
```

Local polynomial model:
```{r}
## model with span=0.15: fitted y
fitted(tt1) # == predict(tt1)
## model with span=0.5: fitted y
fitted(tt2) # == predict(tt2)
```

Compare:
```{r}
nw_box_error <- dist - nw_fitted_box
nw_box_error
nw_gaussian_error <- dist - nw_fitted_gaussian
nw_gaussian_error
local_error1 <- dist - fitted(tt1)
local_error1
local_error2 <- dist - fitted(tt2)
local_error2
## SSE & MSE & std.error
nw_box_sse <-
  sum(nw_box_error ^ 2)
nw_gaussian_sse <- sum(nw_gaussian_error ^ 2)
local_sse1 <- sum(local_error1 ^ 2)
local_sse2 <- sum(local_error2 ^ 2)
nw_box_mse <- nw_box_sse / 50
nw_gaussian_mse <- nw_gaussian_sse / 50
local_mse1 <- local_sse1 / 50
local_mse2 <- local_sse2 / 50
Model <-
  c(
    "Nadaraya-Watson with box kernel",
    "Nadaraya-Watson with gaussian kernel",
    "local polynomial with span=0.15",
    "local polynomial with span=0.5"
  )
SSE <- c(nw_box_sse, nw_gaussian_sse, local_sse1, local_sse2)
MSE <- c(nw_box_mse, nw_gaussian_mse, local_mse1, local_mse2)
std.error <-
  c(sd(nw_box_error),
    sd(nw_gaussian_error),
    sd(local_error1),
    sd(local_error2))
data.frame(Model, SSE, MSE, std.error)
```
Nadaraya-Watson kernel regression model with box kernel fits better.

### (d)
```{r}
plot(speed, dist)
lines(nw_box, col = '#D16BA5', lwd = 2)
lines(nw_gaussian, col = '#86A8E7', lwd = 2)
points(tt1$x, fitted(tt1), col = '#5FFBF1', lwd = 2)
points(tt2$x, fitted(tt2), col = '#E69F00', lwd = 2)
legend(
  x = c(4, 13),
  y = c(95, 120),
  c("Box, Width=1", "Normal, Width=1"),
  col = c('#D16BA5', '#86A8E7'),
  lwd = 1
)
legend(
  x = c(4, 13),
  y = c(60, 90),
  c("Gaussian, span=0.15", "Gaussian, span=0.5"),
  col = c('#5FFBF1', '#E69F00'),
  pch = 'o'
)
```
```{r}
detach(cars)
```

## 3.2
```{r}
library(MASS)
data(galaxies)
```
### Histogram Smoothing:
s is the sample standard deviation and n is the sample zie, then$$ h^* = 3.491sn^{-1/3}$$
```{r}
n <- length(galaxies)
s <- sd(galaxies)
iqr <- IQR(galaxies)
h1 <- 3.491 * s * n ^ {
  -1 / 3
}
nobreaks <- (max(galaxies) - min(galaxies)) / h1
hist(galaxies,
     breaks = round(nobreaks),
     probability = TRUE)
```

Another sensible estimate is obtained by replacing $s$ by the inter-quantile range, IQR, that is $$h^* = 2.6IQR\times n^{-1/3}$$
```{r}
h2 <- 2.6 * iqr * n ^ {
  -1 / 3
}
nobreaks2 <- (max(galaxies) - min(galaxies)) / h2
hist(galaxies,
     breaks = round(nobreaks2),
     probability = TRUE)
```

### Kernel Smoothing:
```{r}
hist(
  galaxies,
  breaks = round(nobreaks),
  probability = TRUE,
  ylim = c(0, 13e-05),
  main = 'Uniform Kernel with h=1700'
)
lines(density(galaxies, kernel = 'rectangular', bw = 1700), col = '#DE3163')

hist(
  galaxies,
  breaks = round(nobreaks),
  probability = TRUE,
  ylim = c(0, 13e-05),
  main = 'Triangle Kernel with h=1800'
)
lines(density(galaxies, kernel = 'triangular', bw = 1800), col = '#FFBF00')

hist(
  galaxies,
  breaks = round(nobreaks),
  probability = TRUE,
  ylim = c(0, 13e-05),
  main = 'Epanechnikov Kernel with h=1900'
)
lines(density(galaxies, kernel = 'epanechnikov', bw = 1900), col = '#6495ED')

hist(
  galaxies,
  breaks = round(nobreaks),
  probability = TRUE,
  ylim = c(0, 13e-05),
  main = 'Gaussian Kernel with h=2000'
)
lines(density(galaxies, kernel = 'gaussian', bw = 2000), col = '#9FE2BF')
```
There are at least 2 peaks of the distribution of velocities. Thus, the multimodality of the distribution of velocities implies the existence of superclusters.

## 3.3
```{r}
library(HSAUR3)
data(foster)
attach(foster)
```

### (a)
```{r}
table(litgen, motgen)
# group means
aggregate(weight, by = list(litgen, motgen), FUN = mean)
# group standard deviations
aggregate(weight, by = list(litgen, motgen), FUN = sd)
```

### (b)
```{r}
library(HH)
interaction2wt(weight ~ litgen * motgen)
```
From the plot in the upper left corner, the slope of different $motgen$ from one $litgen$ type to another $litgen$ type is different. Similarly, from the plot in the lower right corner, the slope of different $litgen$ with respect to $motgen$ is not the same. Therefore, there seems to exist some interaction between $litgen$ and $motgen$. 

### (c)
```{r}
fit1 <- aov(weight ~ litgen * motgen)
fit2 <- aov(weight ~ motgen * litgen)
fit3 <- aov(weight ~ litgen + motgen)
fit4 <- aov(weight ~ motgen + litgen)
summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
```
All the results indicate that the $motgen$ main effect is significant and the $litgen$ main effect is not significant. For analysis with interaction term, the results indicate that there exists some interaction between $motgen$ and $litgen$, but this interaction is not significant(p=0.12005).

### (d)
The dependent variable is assumed to be normally distributed, and have equal variance in each group.  outlier

```{r}
# Normally distributed:
library(car)
qqPlot(
  lm(weight ~ litgen, data = foster),
  simulate = TRUE,
  main = 'Q-Q Plot',
  labels = FALSE
)
# equality of variances
bartlett.test(weight ~ litgen, data = foster)
# outlier
fit <- aov(weight ~ litgen)
outlierTest(fit)
```
Yes. These assumptions are satisfied.

### (e)
```{r}
library(lmPerm)
set.seed(1234)
aovobject <- aovp(weight ~ litgen * motgen, data = foster, perm = "Prob")
summary(aovobject)
### result in (c)
summary(fit1)
```

Compared to the result in (c), the $litgen$ main effect, the $motgen$ main effect and the interaction are all less significant.

```{r}
detach(foster)
```

## 3.4
```{r}
library(ISLR)
data(Default)
```

### (a)
```{r}
summary(Default)
logit_fit <-
  glm(default ~ student + balance + income,
      family = binomial(link = "logit"),
      data = Default)
summary(logit_fit)
```
The estimated standard error for the estimated coefficient of studentYes is $2.363\times10^{-1}$, while the estimated standard error for the estimated coefficient of balance and income are $2.319\times10^{-4}$ and $8.203\times10^{-6}$ respectively.

### (b)
```{r}
boot.fn <- function(formula, data, indices) {
  d <- data[indices, ]
  fit <- glm((formula),
             family = binomial(link = "logit"),
             data = d)
  return(coef(fit))
}
```


### (c)
```{r}
library(boot)
set.seed(1234)
results <-
  boot(
    data = Default,
    statistic = boot.fn,
    R = 1000,
    formula = default ~ student + balance + income
  )
print(results)
```
The estimated standard error for the estimated coefficient of studentYes is $2.390398\times10^{-1}$, while the estimated standard error for the estimated coefficient of balance and income are $2.330627\times10^{-4}$ and $8.595409\times10^{-6}$ respectively.


### (d)
The standard errors obtained by the bootstrap appear to be a quite close to those obtained using the statistical formulas underlying the glm() function. This suggests that the data satisfies the underlying assumptions of a logistic regression model: the responses  $Y_i$  are independent random variables coming from Bernoulli distributions with probabilities $P_i$, and the log-odds corresponding to $P_i$ is a linear combination of the predictors.

















